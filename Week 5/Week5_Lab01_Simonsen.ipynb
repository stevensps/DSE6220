{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22aa9496-24df-445b-8faa-b62586b9fbbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "#Scale up executor memory from 1g\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.executor.memory\", \"2g\")\n",
    "         .appName(\"Week5_Lab01_Simonsen\")\n",
    "         .getOrCreate()\n",
    "        \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c9d732-0843-4074-8486-8b010aeb7f2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78433c32-ad15-4974-8cfd-09b218aea609",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2g'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.executor.memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b3cdcb-b747-4a39-8714-54f1b57d1945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1)\tUse Spark to read in the airport-codes-na and departuredelays datasets.\n",
    "\n",
    "airport_codes_file = \"dbfs:/FileStore/Merrimack/Week_5/airport_codes_na.txt\"\n",
    "codes_df = (spark.read\n",
    "            .format(\"text\")\n",
    "            .option(\"delimiter\", \"\\t\")\n",
    "            .option(\"header\",\"true\")\n",
    "            .csv(airport_codes_file)\n",
    "        )\n",
    "\n",
    "codes_df.createOrReplaceTempView(\"codes_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4070b88-0161-4547-82a1-07554037671c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM codes_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc6aed24-b16c-4571-96a7-6f44ded172a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "departure_delays_file = \"dbfs:/FileStore/Merrimack/Week_5/departuredelays.csv\"\n",
    "\n",
    "dep_delay_df = (spark.read.format(\"csv\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(departure_delays_file)\n",
    "            )\n",
    "\n",
    "dep_delay_df = (dep_delay_df\n",
    "          .withColumn(\"delay\", F.expr(\"CAST(delay as INT) as delay\"))\n",
    "          .withColumn(\"distance\", F.expr(\"CAST(distance as INT) as distance\"))\n",
    "          )\n",
    "\n",
    "dep_delay_df.createOrReplaceTempView(\"dep_delay_na\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fdc940b-b390-47b1-9c27-01cb754e74dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dep_delay_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abfbd000-2669-4cf8-8d15-614ff2df4b08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+---------+-----+-------+----+\n",
      "|date   |delay|distance|origin|destination|City     |State|Country|IATA|\n",
      "+-------+-----+--------+------+-----------+---------+-----+-------+----+\n",
      "|1011245|6    |602     |ABE   |ATL        |Allentown|PA   |USA    |ABE |\n",
      "|1020600|-8   |369     |ABE   |DTW        |Allentown|PA   |USA    |ABE |\n",
      "|1021245|-2   |602     |ABE   |ATL        |Allentown|PA   |USA    |ABE |\n",
      "|1020605|-4   |602     |ABE   |ATL        |Allentown|PA   |USA    |ABE |\n",
      "|1031245|-4   |602     |ABE   |ATL        |Allentown|PA   |USA    |ABE |\n",
      "+-------+-----+--------+------+-----------+---------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2)\tJoin the airport codes dataset to the departuredelays dataset.\n",
    "\n",
    "dep_delay_df.join(codes_df, dep_delay_df['origin'] == codes_df['IATA'], 'inner')\\\n",
    "    .show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4170af9c-69d9-4e09-89be-41f52e2db115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2g\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe spark join that woud be best for this would be a Broadcast Hash Join. Given that the airport-codes-na dataset is relatively small in comparison to the departuredelays dataset, this type of join makes sense. By using this type of join, spark broadcasts the small dataframe (airport-codes-na) to each worker, where it is then joined to the larger dataframe (departuredelays) at each worker. This eliminates the need for complex shuffles, and is much less of an expensive operation than say, a shuffle-sort merge join. \\n\\nFor this type of a join, I did not need to set any configuration parameters. Per the above and in checking my executors tab within the Spark UI, I have minimal tasks (currently 10 at the time of writing this) that I am running on 1 executor, which is also the driver. I have 4 cores allocated to the driver. So, in this case, the number of cores multipled by number of executors is 4, which is already set to the correct value in listing the default.parallelism value above. If I noticed my jobs were creating \"resource-hungry\" workloads by creating a lot of executors, I could set the cluster value for spark.dynamicAllocation.maxExecutors to say, 5 for example.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3)\tWhat type of Spark join would be best for this? Do you need to set any configuration parameters to complete the join? \n",
    "\n",
    "print(sc.defaultParallelism)\n",
    "print(spark.conf.get('spark.executor.memory'))\n",
    "\n",
    "'''\n",
    "The spark join that woud be best for this would be a Broadcast Hash Join. Given that the airport-codes-na dataset is relatively small in comparison to the departuredelays dataset, this type of join makes sense. By using this type of join, spark broadcasts the small dataframe (airport-codes-na) to each worker, where it is then joined to the larger dataframe (departuredelays) at each worker. This eliminates the need for complex shuffles, and is much less of an expensive operation than say, a shuffle-sort merge join. \n",
    "\n",
    "For this type of a join, I did not need to set any configuration parameters. Per the above and in checking my executors tab within the Spark UI, I have minimal tasks (currently 10 at the time of writing this) that I am running on 1 executor, which is also the driver. I have 4 cores allocated to the driver. So, in this case, the number of cores multipled by number of executors is 4, which is already set to the correct value in listing the default.parallelism value above. If I noticed my jobs were creating \"resource-hungry\" workloads by creating a lot of executors, I could set the cluster value for spark.dynamicAllocation.maxExecutors to say, 5 for example.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53bec7fd-b698-412b-a5e7-10b6009e40bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Origin|count|\n",
      "+------+-----+\n",
      "|ATL   |91484|\n",
      "|DFW   |68482|\n",
      "|ORD   |64228|\n",
      "|LAX   |54086|\n",
      "|DEN   |53148|\n",
      "|IAH   |43361|\n",
      "|PHX   |40155|\n",
      "|SFO   |39483|\n",
      "|LAS   |33107|\n",
      "|CLT   |28402|\n",
      "+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) What would a logical partition for this dataset be? You can use methods like aggregation and count to determine if there are any logical partitions. \n",
    "(dep_delay_df\n",
    "    .groupBy(\"Origin\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .show(n=10, truncate=False)\n",
    ")\n",
    "\n",
    "#Based on my code, origin would be a logical partition for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb1be89-0f4d-42c7-9295-e59937d27c5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nORC: One of the benefits to using ORC is the compression to shrink file size. Of all the most commonly used data storage formats (Parquet, ORC, AVRO, Text/CSV), ORC uses the best compression algorithm. Additionally, ORC is commonly known to perform very within Hadoop/Hive envrionments with large amount of data stored on disk. However, this also could be viewed as a drawback to using ORC. Many organizations are tranisitioning to a cloud-based data architecture, and ORC does not generally perform as well as Parquet in a Spark environment. Additionally, the ORC does not offer the schema resolution flexibility of AVRO, and it is generally a bit more difficult to change and adjust the schema within the data.\\n\\nParquet: Similar to ORC files, Parquet also uses good compression algorithms in comparison across all commonly used data storage formats. While Parquet does not have quite as good of file compression as ORC, it generally performs best in Spark envrionments. Given the rise in popularity of Spark because of its ability to utilize memory-based computation as opposed to disk, this is appealing for many organizations. However, outside of the Spark envrionment, Parquet data storage formatting may not be the best performing option. Additionally, Spark also does not offer the schema resolution flexibility that AVRO does. All things considered, so long as Parquet continues to perform best in Spark of all the commonly used data storage formats, it will continue to be a widely popular and commonly used option.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5)\tWrite the combined data back to your data directory using the partition key you determined in the previous step. \n",
    "# a.\tWrite the data in a directory using parquet. \n",
    "# b.\tWrite the data in a directory using ORC. \n",
    "# c.\tWhat are the benefits/disadvantages of using ORC or Parquet for reading/writing data? \n",
    "\n",
    "\n",
    "#a\n",
    "path = \"dbfs:/FileStore/Merrimack/Week_5/parquet_data\"\n",
    "\n",
    "(dep_delay_df\n",
    " .repartition(\"origin\")\n",
    " .write.format('parquet')\n",
    " .partitionBy('origin')\n",
    " .mode('overwrite')\n",
    " .option(\"header\", \"true\")\n",
    " .save(path)\n",
    ")\n",
    "\n",
    "#b\n",
    "path_orc = \"dbfs:/FileStore/Merrimack/Week_5/orc_data\"\n",
    "\n",
    "(dep_delay_df\n",
    " .repartition(\"origin\")\n",
    " .write.format('orc')\n",
    " .partitionBy('origin')\n",
    " .mode('overwrite')\n",
    " .option(\"header\", \"true\")\n",
    " .save(path_orc)\n",
    ")\n",
    "\n",
    "#c\n",
    "'''\n",
    "ORC: One of the benefits to using ORC is the compression to shrink file size. Of all the most commonly used data storage formats (Parquet, ORC, AVRO, Text/CSV), ORC uses the best compression algorithm. Additionally, ORC is commonly known to perform very within Hadoop/Hive envrionments with large amount of data stored on disk. However, this also could be viewed as a drawback to using ORC. Many organizations are tranisitioning to a cloud-based data architecture, and ORC does not generally perform as well as Parquet in a Spark environment. Additionally, the ORC does not offer the schema resolution flexibility of AVRO, and it is generally a bit more difficult to change and adjust the schema within the data.\n",
    "\n",
    "Parquet: Similar to ORC files, Parquet also uses good compression algorithms in comparison across all commonly used data storage formats. While Parquet does not have quite as good of file compression as ORC, it generally performs best in Spark envrionments. Given the rise in popularity of Spark because of its ability to utilize memory-based computation as opposed to disk, this is appealing for many organizations. However, outside of the Spark envrionment, Parquet data storage formatting may not be the best performing option. Additionally, Spark also does not offer the schema resolution flexibility that AVRO does. All things considered, so long as Parquet continues to perform best in Spark of all the commonly used data storage formats, it will continue to be a widely popular and commonly used option.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79e733b-3ffd-4553-b5b9-71ae1fbb7684",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nI modified the executor memory to 2GB instead of the default value of 1GB assigned. Per the above, the other configuration values I checked such as default parallelism and number of executors seemed appropriately configured for the job and tasks I was running. \\n\\nAs this process scales up, I would expect to have to adjust certain parameters to optimize my job. First, I would expect to adjust the min and max number of executors to be sure I am appropriately scaling for the job. More than likely, I would want to set the spark.dynamicAllocation.minExecutors to 2 to be sure I at least have 2 workers executing a minimum. I would also want to set the spark.dynamicAllocation.maxExecutors to a higher value, say 20 at the most, to avoid Databricks allocating an infinite number of workers to the job, which would be very costly. Setting the spark.dynamicAllocation.executorIdleTimeout to a value of a few minutes, potentially 2, would also be a good idea to avoid idle workers running without tasks. A lot of this may also depend on how many pertitions I'm bringing in, and how resource intensive the jobs I'm running are (i.e. aggregations, calculations, etc.). Finally, it may be necessary to increase values if shuffles are occuring, such as increasing the spark.shuffle.unsafe.file.output.buffer to adjust for merging files, increasing the value of spark.shuffle.registration.timeout to increase the time until timeout, and also increaseing the spark.shuffle.file.buffer to allow Spark to buffer more before writing data back to disk.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6)\tWhat if any configuration changes did you set to tune your job?\n",
    "# a.\tAs this process scales up, what parameters do you think you would need to optimize your job? \n",
    "\n",
    "'''\n",
    "I modified the executor memory to 2GB instead of the default value of 1GB assigned. Per the above, the other configuration values I checked such as default parallelism and number of executors seemed appropriately configured for the job and tasks I was running. \n",
    "\n",
    "As this process scales up, I would expect to have to adjust certain parameters to optimize my job. First, I would expect to adjust the min and max number of executors to be sure I am appropriately scaling for the job. More than likely, I would want to set the spark.dynamicAllocation.minExecutors to 2 to be sure I at least have 2 workers executing a minimum. I would also want to set the spark.dynamicAllocation.maxExecutors to a higher value, say 20 at the most, to avoid Databricks allocating an infinite number of workers to the job, which would be very costly. Setting the spark.dynamicAllocation.executorIdleTimeout to a value of a few minutes, potentially 2, would also be a good idea to avoid idle workers running without tasks. A lot of this may also depend on how many pertitions I'm bringing in, and how resource intensive the jobs I'm running are (i.e. aggregations, calculations, etc.). Finally, it may be necessary to increase values if shuffles are occuring, such as increasing the spark.shuffle.unsafe.file.output.buffer to adjust for merging files, increasing the value of spark.shuffle.registration.timeout to increase the time until timeout, and also increaseing the spark.shuffle.file.buffer to allow Spark to buffer more before writing data back to disk.\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Week5_Lab01_Simonsen",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
